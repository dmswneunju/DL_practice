{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6d4b7b",
   "metadata": {},
   "source": [
    "## 딥러닝의 여러 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8857cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. module import'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2734cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.4.0 Device: cuda\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을설계할 때 활용하는 장비 확인'''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, 'Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f51beb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "''' 3. MNIST 데이터 다운로드(train, test set 분리)'''\n",
    "train_dataset = datasets.MNIST(root = '../data/MNIST',\n",
    "                              train=True,\n",
    "                              download=True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = '../data/MNIST',\n",
    "                              train=False,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f94bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인(1)'''\n",
    "for (x_train, y_train) in train_loader:\n",
    "    print('x_train:', x_train.size(), 'type:', x_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9a9dbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 5. 데이터 확인(2)'''\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_train[i,:,:,:].numpy().reshape(28,28), cmap='gray_r')\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3b6df",
   "metadata": {},
   "source": [
    "## 3-1.Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fbad55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. MLP모델 설계-drop out 설정'''\n",
    "class Net(nn.Module): #nn.Module클래스를 상속받는 Net클래스 정의\n",
    "    def __init__(self): #Net인스턴스 생성했을 대 지니게 되는 성질 정의\n",
    "        super(Net, self).__init__() #nn.Module내에 있는 메스드 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) #첫번째 fully connected layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "        #Net클래스를 이용해 설계한 MLP모델의 forward propagation 정의\n",
    "        #설계한 MLP모델에 데이터를 입력했을 때 output계산하기까지의 과정 나열\n",
    "    def forward(self, x): \n",
    "        # MLP모델은 1차원의 벡터 값을 입력으로 받을 수 있다. 그러나 MNIST 이미지데이터는 2차원.\n",
    "        # 2차원->1차원 => view메서드 이용해여 784크기의 1차원 데이터로 변환해 진행\n",
    "        # 2차원의 데이터를 1차원으로 펼친다. flatten\n",
    "        x = x.view(-1,28*28) \n",
    "        \n",
    "        # __init__의 첫번째 fully connected layer에 1차원으로 펼친 이미지 데이터 통과\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # 두번째 fully connected layer의 input으로 계산.\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        # 두번째 fully connected layer에서 sigmoid함수를 이용해 계산 된 결괏값 통과\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # 세번째 fully connected layer의 input\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        # 세번째 fully connected layer에서 sigmoid함수를 이용해 계산 된 결괏값 통과\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # 최종 output계산. 0~9까지 총 10가지 중 하나로 분류 => softmax를 이용해 확률 값 계산\n",
    "        # softmax가 아닌 log_softmax 사용한 이유:역전파 진행할때 loss에 대한 gradient값을 원할하게 계산가능.\n",
    "        # 로그함수의 기울기는 부드럽게 변화\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # 최종 계산된 x값을 output으로 반환\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee8b63",
   "metadata": {},
   "source": [
    "## 3-2.Dropout+ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0e77a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. MLP모델 설계'''\n",
    "class Net(nn.Module): #nn.Module클래스를 상속받는 Net클래스 정의\n",
    "    def __init__(self): #Net인스턴스 생성했을 대 지니게 되는 성질 정의\n",
    "        super(Net, self).__init__() #nn.Module내에 있는 메스드 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) #첫번째 fully connected layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "        #Net클래스를 이용해 설계한 MLP모델의 forward propagation 정의\n",
    "        #설계한 MLP모델에 데이터를 입력했을 때 output계산하기까지의 과정 나열\n",
    "    def forward(self, x): \n",
    "        # MLP모델은 1차원의 벡터 값을 입력으로 받을 수 있다. 그러나 MNIST 이미지데이터는 2차원.\n",
    "        # 2차원->1차원 => view메서드 이용해여 784크기의 1차원 데이터로 변환해 진행\n",
    "        # 2차원의 데이터를 1차원으로 펼친다. flatten\n",
    "        x = x.view(-1,28*28) \n",
    "        \n",
    "        # __init__의 첫번째 fully connected layer에 1차원으로 펼친 이미지 데이터 통과\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # 세번째 fully connected layer의 input\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # 최종 output계산. 0~9까지 총 10가지 중 하나로 분류 => softmax를 이용해 확률 값 계산\n",
    "        # softmax가 아닌 log_softmax 사용한 이유:역전파 진행할때 loss에 대한 gradient값을 원할하게 계산가능.\n",
    "        # 로그함수의 기울기는 부드럽게 변화\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # 최종 계산된 x값을 output으로 반환\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54270fa6",
   "metadata": {},
   "source": [
    "## 3-3. Dropout + ReLU + Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5eebcfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. MLP모델 설계'''\n",
    "class Net(nn.Module): #nn.Module클래스를 상속받는 Net클래스 정의\n",
    "    def __init__(self): #Net인스턴스 생성했을 대 지니게 되는 성질 정의\n",
    "        super(Net, self).__init__() #nn.Module내에 있는 메스드 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) #첫번째 fully connected layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        #Net클래스를 이용해 설계한 MLP모델의 forward propagation 정의\n",
    "        #설계한 MLP모델에 데이터를 입력했을 때 output계산하기까지의 과정 나열\n",
    "    def forward(self, x): \n",
    "        # MLP모델은 1차원의 벡터 값을 입력으로 받을 수 있다. 그러나 MNIST 이미지데이터는 2차원.\n",
    "        # 2차원->1차원 => view메서드 이용해여 784크기의 1차원 데이터로 변환해 진행\n",
    "        # 2차원의 데이터를 1차원으로 펼친다. flatten\n",
    "        x = x.view(-1,28*28) \n",
    "        \n",
    "        # __init__의 첫번째 fully connected layer에 1차원으로 펼친 이미지 데이터 통과\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        # 두번째 fully connected layer의 input으로 계산.\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        # 세번째 fully connected layer의 input\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02d3bad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Object Function 설정'''\n",
    "model = Net().to(DEVICE)\n",
    "# 역전파를 이용해 파라미터를 업데이터 할 대 이용하는 optimizer를 정의\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d4d0c",
   "metadata": {},
   "source": [
    "## 3-4. Dropout + ReLU + Batch normalization + He Uniform Initialization\n",
    "He Uniform Initialization : Xavier Initialization은 ReLI함수를 사용할때 비효율적이라는 점을 보완한 초기화 기법\n",
    "## 3-5. Dropout + ReLU + Batch normalization + He Uniform Initialization + Adam(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83336ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 7. Optimizer, Object Function 설정'''\n",
    "# weight, bias 등 딥러닝 모델에서 초깃값으로 설정되는 요소에 대한 모듈인 init를 임포트\n",
    "import torch.nn.init as init\n",
    "\n",
    "# MLP모델 내의 weight를 초기화할 부분 설정\n",
    "def weight_init(m):\n",
    "    # MLP모델을 구성하고 있는 파라미터 중 nn.Linear에 해당하는 파라미터 값에 대해서만 지정\n",
    "    if isinstance(m, nn.Linear): # m이 nn.Linear형인지\n",
    "        init.kaiming_uniform_(m.weight.data) #kaiming_uniform_ : He Initialization\n",
    "        \n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "# 역전파를 이용해 파라미터를 업데이터 할 대 이용하는 optimizer를 정의\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7a34b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의'''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    #trian_loader에는 학습에 이용되는 이미지 데이터와 레이블 데이터가 미니배치 단위로 묶여 저장되어있음.\n",
    "    #train_loader내에 mini-batch단위로 저장된 데이터를 순서대로 이용해 MLP모형에 학습\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):\n",
    "        # print(batch_idx) : 200\n",
    "        #미니배치 내에 있는 이미지 데이터와 매칭된 레이블 데이터도 MLP모델을 학습시키기 위해 기존에 정의한 장비에 할당\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        \n",
    "        # 과거에 이용한 미니배치 내에 있는 이미지데이터와 레이블데이터를 바탕으로 계산된 Loss의 Gradient값이 optimizer에 할당돼 있으므로 \n",
    "        # optimizer의 gradient를 초기화\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        \n",
    "        #계산된 output과 장비에 할당된 레이블 데이터를 기존에 정의한 cross entropy를 이용해 loss값 계산\n",
    "        loss = criterion(output,label)\n",
    "        \n",
    "        #loss값을 계산한 결과를 바탕으로 역전파를 이용해 계산된 gradient값을 각 파라미터에 할당\n",
    "        loss.backward()\n",
    "        \n",
    "        #각 파라미터에 할당된 gradient값을 이용해 파라미터값 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "            Epoch, batch_idx*len(image),\n",
    "            len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a2bef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의'''\n",
    "def evaluate(model, test_loader):\n",
    "    #학습 과정 또는 학습이 완료된 MLP모델을 학습 상태가 아닌, 평가 상태로 지정\n",
    "    model.eval()\n",
    "    \n",
    "    #test_loader내의 데이터를 이용해 loss값을 계산하기 위해 test_loss를 0으로 임시설정\n",
    "    test_loss = 0\n",
    "    \n",
    "    #학습 과정 또는 학습이 완료된 MLP모델이 올바른 class로 분류한 경우를 세기 위해 correct=0으로 임시 설정\n",
    "    correct = 0\n",
    "    \n",
    "    #모델 평가단계에서 gradient를 통해 파라미터 값이 업데이트 되는 현상을 방지하기 위해 torch.no_grad()메서드 이용해 gradient흐름 억제\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            \n",
    "            #기존에 정의한 crossentropy를 이용해 loss값을 계산한 결괏값을 test_loss에 더해 업데이트\n",
    "            test_loss += criterion(output, label).item()\n",
    "            \n",
    "            #output값은 크기가 10인 벡터 값. 계산된 벡터 값 내 가장 큰 값인 위치에 대해 해당 위치에 대응하는 클래스로 예측했다고 판단.\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            \n",
    "            #모델이 최종으로 예측한 클래스의 값(prediction)과 실제 레이블(label)이 의미하는 클래스가 맞으면 correct에 더해 올바르게 예측한 횟수 저장\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "    #현재까지 계산된 test_loss값을 test_loader내에 존재하는 미니배치 개수만큼 나눠 평균 loss값으로 계산\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    #test_loader데이터 중 얼마나 맞췄는지를 계산해 정확도를 계산\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) #len(test_loader.dataset):10000\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e8f6e",
   "metadata": {},
   "source": [
    "## 3-1.epochs=10, sigmoid+dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285e1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.393649\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 2.459405\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 2.362060\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 2.283095\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 2.360279\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 2.396481\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 2.304133\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 2.281083\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 2.314523\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 2.241969\n",
      "\n",
      "[EPOCH:1], \tTest Loss: 0.0713, \tTest Accuracy: 15.02 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 2.291219\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 2.293644\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 2.259175\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 2.260362\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 2.249936\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 2.311914\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 2.074335\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 2.083643\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 2.128265\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 2.076288\n",
      "\n",
      "[EPOCH:2], \tTest Loss: 0.0630, \tTest Accuracy: 37.51 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 1.932966\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 1.932397\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 1.920705\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 1.831211\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 1.772467\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 1.693843\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 1.527012\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 1.452714\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 1.572952\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 1.404862\n",
      "\n",
      "[EPOCH:3], \tTest Loss: 0.0386, \tTest Accuracy: 60.51 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 1.536186\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 1.361418\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 1.489338\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 1.247308\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 1.291938\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 1.131321\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.985700\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 1.178569\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.991072\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.857483\n",
      "\n",
      "[EPOCH:4], \tTest Loss: 0.0273, \tTest Accuracy: 71.50 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.894803\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 1.034462\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.827182\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 1.087394\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.845128\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.797025\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.898634\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.831460\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 1.127187\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.872794\n",
      "\n",
      "[EPOCH:5], \tTest Loss: 0.0237, \tTest Accuracy: 75.77 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.961532\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.716380\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.852834\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.853355\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.560714\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 1.577468\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.622923\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.841634\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.825696\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.927748\n",
      "\n",
      "[EPOCH:6], \tTest Loss: 0.0210, \tTest Accuracy: 79.45 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 1.112878\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.795329\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 1.014936\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 1.252336\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.809911\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.832427\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.420917\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.877474\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.616071\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.561539\n",
      "\n",
      "[EPOCH:7], \tTest Loss: 0.0184, \tTest Accuracy: 82.90 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.656783\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.877509\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 1.100035\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.676471\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 1.222992\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 1.132227\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.721216\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.603599\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.617339\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.487904\n",
      "\n",
      "[EPOCH:8], \tTest Loss: 0.0163, \tTest Accuracy: 84.56 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.627692\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.718423\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.514654\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.694737\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.473958\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.619390\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.675476\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.676540\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.805036\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.746385\n",
      "\n",
      "[EPOCH:9], \tTest Loss: 0.0149, \tTest Accuracy: 85.99 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.398627\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.433351\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.669282\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.499030\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.676977\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.585558\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.439056\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 1.042884\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.598724\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.660187\n",
      "\n",
      "[EPOCH:10], \tTest Loss: 0.0139, \tTest Accuracy: 86.68 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP학습을 실행하면서 trian, test set의 loss 및 test set accuracy를 확인'''\n",
    "for Epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH:{}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(\n",
    "    Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560edece",
   "metadata": {},
   "source": [
    "## 3-1.epochs=30, sigmoid+dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbc8d4a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.435961\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 2.406326\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 2.338543\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 2.320160\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 2.287632\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 2.327087\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 2.343685\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 2.285419\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 2.303885\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 2.290537\n",
      "\n",
      "[EPOCH:1], \tTest Loss: 0.0713, \tTest Accuracy: 10.09 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 2.303993\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 2.235222\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 2.310011\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 2.218569\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 2.242838\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 2.285932\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 2.281090\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 2.186352\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 2.132408\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 1.921089\n",
      "\n",
      "[EPOCH:2], \tTest Loss: 0.0605, \tTest Accuracy: 39.41 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 2.068668\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 1.771406\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 1.931139\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 1.948146\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 1.608805\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 1.491313\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 1.726569\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 1.633663\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 1.439530\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 1.420536\n",
      "\n",
      "[EPOCH:3], \tTest Loss: 0.0391, \tTest Accuracy: 58.66 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 1.492025\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 1.387689\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 1.227510\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 1.096663\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 1.241517\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 1.178474\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.966649\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 1.071797\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 1.054080\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.935775\n",
      "\n",
      "[EPOCH:4], \tTest Loss: 0.0284, \tTest Accuracy: 70.57 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 1.185020\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 1.014641\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 1.056650\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.946989\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.771659\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.857376\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.930912\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.989959\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 1.200247\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.938075\n",
      "\n",
      "[EPOCH:5], \tTest Loss: 0.0241, \tTest Accuracy: 74.55 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.740385\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.771501\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.806043\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.864069\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.988875\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 1.055043\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 1.153328\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.791410\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 1.042675\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.845758\n",
      "\n",
      "[EPOCH:6], \tTest Loss: 0.0211, \tTest Accuracy: 80.03 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.829465\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.666521\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.733355\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.592450\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.745386\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.950680\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.755705\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.779413\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.604149\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.859874\n",
      "\n",
      "[EPOCH:7], \tTest Loss: 0.0184, \tTest Accuracy: 82.40 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.747311\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.686508\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.562811\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.619616\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.420549\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.548118\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.496966\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.690715\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.728736\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.734751\n",
      "\n",
      "[EPOCH:8], \tTest Loss: 0.0162, \tTest Accuracy: 84.90 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.617635\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.443845\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.486547\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.644460\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 1.074928\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.511639\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.497440\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.638109\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.760289\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.485839\n",
      "\n",
      "[EPOCH:9], \tTest Loss: 0.0148, \tTest Accuracy: 85.95 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.534350\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.629764\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.712028\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.430655\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.768458\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.347116\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.365496\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.708206\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.584084\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.833281\n",
      "\n",
      "[EPOCH:10], \tTest Loss: 0.0138, \tTest Accuracy: 86.92 %\n",
      "\n",
      "Train Epoch: 11 [0/60000(0%)]\tTrain Loss: 0.748462\n",
      "Train Epoch: 11 [6400/60000(11%)]\tTrain Loss: 0.720160\n",
      "Train Epoch: 11 [12800/60000(21%)]\tTrain Loss: 0.920257\n",
      "Train Epoch: 11 [19200/60000(32%)]\tTrain Loss: 0.665675\n",
      "Train Epoch: 11 [25600/60000(43%)]\tTrain Loss: 0.513489\n",
      "Train Epoch: 11 [32000/60000(53%)]\tTrain Loss: 0.599088\n",
      "Train Epoch: 11 [38400/60000(64%)]\tTrain Loss: 0.478163\n",
      "Train Epoch: 11 [44800/60000(75%)]\tTrain Loss: 0.589861\n",
      "Train Epoch: 11 [51200/60000(85%)]\tTrain Loss: 0.712699\n",
      "Train Epoch: 11 [57600/60000(96%)]\tTrain Loss: 1.030272\n",
      "\n",
      "[EPOCH:11], \tTest Loss: 0.0132, \tTest Accuracy: 87.40 %\n",
      "\n",
      "Train Epoch: 12 [0/60000(0%)]\tTrain Loss: 0.308962\n",
      "Train Epoch: 12 [6400/60000(11%)]\tTrain Loss: 0.673282\n",
      "Train Epoch: 12 [12800/60000(21%)]\tTrain Loss: 0.712491\n",
      "Train Epoch: 12 [19200/60000(32%)]\tTrain Loss: 0.478565\n",
      "Train Epoch: 12 [25600/60000(43%)]\tTrain Loss: 0.604698\n",
      "Train Epoch: 12 [32000/60000(53%)]\tTrain Loss: 0.635606\n",
      "Train Epoch: 12 [38400/60000(64%)]\tTrain Loss: 0.821265\n",
      "Train Epoch: 12 [44800/60000(75%)]\tTrain Loss: 0.489888\n",
      "Train Epoch: 12 [51200/60000(85%)]\tTrain Loss: 0.766487\n",
      "Train Epoch: 12 [57600/60000(96%)]\tTrain Loss: 0.386756\n",
      "\n",
      "[EPOCH:12], \tTest Loss: 0.0126, \tTest Accuracy: 87.91 %\n",
      "\n",
      "Train Epoch: 13 [0/60000(0%)]\tTrain Loss: 0.562225\n",
      "Train Epoch: 13 [6400/60000(11%)]\tTrain Loss: 0.559838\n",
      "Train Epoch: 13 [12800/60000(21%)]\tTrain Loss: 0.508558\n",
      "Train Epoch: 13 [19200/60000(32%)]\tTrain Loss: 0.432958\n",
      "Train Epoch: 13 [25600/60000(43%)]\tTrain Loss: 0.445835\n",
      "Train Epoch: 13 [32000/60000(53%)]\tTrain Loss: 0.793055\n",
      "Train Epoch: 13 [38400/60000(64%)]\tTrain Loss: 0.539911\n",
      "Train Epoch: 13 [44800/60000(75%)]\tTrain Loss: 0.491748\n",
      "Train Epoch: 13 [51200/60000(85%)]\tTrain Loss: 0.142192\n",
      "Train Epoch: 13 [57600/60000(96%)]\tTrain Loss: 0.413924\n",
      "\n",
      "[EPOCH:13], \tTest Loss: 0.0122, \tTest Accuracy: 88.43 %\n",
      "\n",
      "Train Epoch: 14 [0/60000(0%)]\tTrain Loss: 0.660591\n",
      "Train Epoch: 14 [6400/60000(11%)]\tTrain Loss: 0.551520\n",
      "Train Epoch: 14 [12800/60000(21%)]\tTrain Loss: 0.390992\n",
      "Train Epoch: 14 [19200/60000(32%)]\tTrain Loss: 0.599594\n",
      "Train Epoch: 14 [25600/60000(43%)]\tTrain Loss: 0.410149\n",
      "Train Epoch: 14 [32000/60000(53%)]\tTrain Loss: 0.360098\n",
      "Train Epoch: 14 [38400/60000(64%)]\tTrain Loss: 0.741070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [44800/60000(75%)]\tTrain Loss: 0.698491\n",
      "Train Epoch: 14 [51200/60000(85%)]\tTrain Loss: 0.367450\n",
      "Train Epoch: 14 [57600/60000(96%)]\tTrain Loss: 0.269219\n",
      "\n",
      "[EPOCH:14], \tTest Loss: 0.0117, \tTest Accuracy: 88.87 %\n",
      "\n",
      "Train Epoch: 15 [0/60000(0%)]\tTrain Loss: 0.393330\n",
      "Train Epoch: 15 [6400/60000(11%)]\tTrain Loss: 0.731158\n",
      "Train Epoch: 15 [12800/60000(21%)]\tTrain Loss: 0.869909\n",
      "Train Epoch: 15 [19200/60000(32%)]\tTrain Loss: 0.526210\n",
      "Train Epoch: 15 [25600/60000(43%)]\tTrain Loss: 0.384606\n",
      "Train Epoch: 15 [32000/60000(53%)]\tTrain Loss: 0.338612\n",
      "Train Epoch: 15 [38400/60000(64%)]\tTrain Loss: 0.543727\n",
      "Train Epoch: 15 [44800/60000(75%)]\tTrain Loss: 0.610306\n",
      "Train Epoch: 15 [51200/60000(85%)]\tTrain Loss: 0.734767\n",
      "Train Epoch: 15 [57600/60000(96%)]\tTrain Loss: 0.753845\n",
      "\n",
      "[EPOCH:15], \tTest Loss: 0.0114, \tTest Accuracy: 89.00 %\n",
      "\n",
      "Train Epoch: 16 [0/60000(0%)]\tTrain Loss: 0.711632\n",
      "Train Epoch: 16 [6400/60000(11%)]\tTrain Loss: 0.420075\n",
      "Train Epoch: 16 [12800/60000(21%)]\tTrain Loss: 0.304257\n",
      "Train Epoch: 16 [19200/60000(32%)]\tTrain Loss: 0.264519\n",
      "Train Epoch: 16 [25600/60000(43%)]\tTrain Loss: 0.201191\n",
      "Train Epoch: 16 [32000/60000(53%)]\tTrain Loss: 0.248053\n",
      "Train Epoch: 16 [38400/60000(64%)]\tTrain Loss: 0.716051\n",
      "Train Epoch: 16 [44800/60000(75%)]\tTrain Loss: 0.436600\n",
      "Train Epoch: 16 [51200/60000(85%)]\tTrain Loss: 0.564911\n",
      "Train Epoch: 16 [57600/60000(96%)]\tTrain Loss: 0.442500\n",
      "\n",
      "[EPOCH:16], \tTest Loss: 0.0111, \tTest Accuracy: 89.32 %\n",
      "\n",
      "Train Epoch: 17 [0/60000(0%)]\tTrain Loss: 0.618334\n",
      "Train Epoch: 17 [6400/60000(11%)]\tTrain Loss: 0.295737\n",
      "Train Epoch: 17 [12800/60000(21%)]\tTrain Loss: 0.433615\n",
      "Train Epoch: 17 [19200/60000(32%)]\tTrain Loss: 0.622490\n",
      "Train Epoch: 17 [25600/60000(43%)]\tTrain Loss: 0.313919\n",
      "Train Epoch: 17 [32000/60000(53%)]\tTrain Loss: 0.462462\n",
      "Train Epoch: 17 [38400/60000(64%)]\tTrain Loss: 0.573154\n",
      "Train Epoch: 17 [44800/60000(75%)]\tTrain Loss: 0.321596\n",
      "Train Epoch: 17 [51200/60000(85%)]\tTrain Loss: 0.736890\n",
      "Train Epoch: 17 [57600/60000(96%)]\tTrain Loss: 0.326261\n",
      "\n",
      "[EPOCH:17], \tTest Loss: 0.0108, \tTest Accuracy: 89.51 %\n",
      "\n",
      "Train Epoch: 18 [0/60000(0%)]\tTrain Loss: 0.415931\n",
      "Train Epoch: 18 [6400/60000(11%)]\tTrain Loss: 0.472865\n",
      "Train Epoch: 18 [12800/60000(21%)]\tTrain Loss: 0.399260\n",
      "Train Epoch: 18 [19200/60000(32%)]\tTrain Loss: 0.270407\n",
      "Train Epoch: 18 [25600/60000(43%)]\tTrain Loss: 0.464930\n",
      "Train Epoch: 18 [32000/60000(53%)]\tTrain Loss: 0.567840\n",
      "Train Epoch: 18 [38400/60000(64%)]\tTrain Loss: 0.423717\n",
      "Train Epoch: 18 [44800/60000(75%)]\tTrain Loss: 0.376939\n",
      "Train Epoch: 18 [51200/60000(85%)]\tTrain Loss: 0.580019\n",
      "Train Epoch: 18 [57600/60000(96%)]\tTrain Loss: 0.480503\n",
      "\n",
      "[EPOCH:18], \tTest Loss: 0.0106, \tTest Accuracy: 89.86 %\n",
      "\n",
      "Train Epoch: 19 [0/60000(0%)]\tTrain Loss: 0.142094\n",
      "Train Epoch: 19 [6400/60000(11%)]\tTrain Loss: 0.366621\n",
      "Train Epoch: 19 [12800/60000(21%)]\tTrain Loss: 0.463563\n",
      "Train Epoch: 19 [19200/60000(32%)]\tTrain Loss: 0.240105\n",
      "Train Epoch: 19 [25600/60000(43%)]\tTrain Loss: 0.655263\n",
      "Train Epoch: 19 [32000/60000(53%)]\tTrain Loss: 0.432579\n",
      "Train Epoch: 19 [38400/60000(64%)]\tTrain Loss: 0.436870\n",
      "Train Epoch: 19 [44800/60000(75%)]\tTrain Loss: 0.474626\n",
      "Train Epoch: 19 [51200/60000(85%)]\tTrain Loss: 0.557237\n",
      "Train Epoch: 19 [57600/60000(96%)]\tTrain Loss: 0.501107\n",
      "\n",
      "[EPOCH:19], \tTest Loss: 0.0103, \tTest Accuracy: 89.96 %\n",
      "\n",
      "Train Epoch: 20 [0/60000(0%)]\tTrain Loss: 0.379812\n",
      "Train Epoch: 20 [6400/60000(11%)]\tTrain Loss: 0.439053\n",
      "Train Epoch: 20 [12800/60000(21%)]\tTrain Loss: 0.435017\n",
      "Train Epoch: 20 [19200/60000(32%)]\tTrain Loss: 0.210955\n",
      "Train Epoch: 20 [25600/60000(43%)]\tTrain Loss: 0.289885\n",
      "Train Epoch: 20 [32000/60000(53%)]\tTrain Loss: 0.395470\n",
      "Train Epoch: 20 [38400/60000(64%)]\tTrain Loss: 0.329870\n",
      "Train Epoch: 20 [44800/60000(75%)]\tTrain Loss: 0.496147\n",
      "Train Epoch: 20 [51200/60000(85%)]\tTrain Loss: 0.378845\n",
      "Train Epoch: 20 [57600/60000(96%)]\tTrain Loss: 0.284654\n",
      "\n",
      "[EPOCH:20], \tTest Loss: 0.0101, \tTest Accuracy: 90.34 %\n",
      "\n",
      "Train Epoch: 21 [0/60000(0%)]\tTrain Loss: 0.313949\n",
      "Train Epoch: 21 [6400/60000(11%)]\tTrain Loss: 0.238916\n",
      "Train Epoch: 21 [12800/60000(21%)]\tTrain Loss: 0.290671\n",
      "Train Epoch: 21 [19200/60000(32%)]\tTrain Loss: 0.511326\n",
      "Train Epoch: 21 [25600/60000(43%)]\tTrain Loss: 0.625468\n",
      "Train Epoch: 21 [32000/60000(53%)]\tTrain Loss: 0.538592\n",
      "Train Epoch: 21 [38400/60000(64%)]\tTrain Loss: 0.455958\n",
      "Train Epoch: 21 [44800/60000(75%)]\tTrain Loss: 0.450344\n",
      "Train Epoch: 21 [51200/60000(85%)]\tTrain Loss: 0.362763\n",
      "Train Epoch: 21 [57600/60000(96%)]\tTrain Loss: 0.375730\n",
      "\n",
      "[EPOCH:21], \tTest Loss: 0.0099, \tTest Accuracy: 90.65 %\n",
      "\n",
      "Train Epoch: 22 [0/60000(0%)]\tTrain Loss: 0.896423\n",
      "Train Epoch: 22 [6400/60000(11%)]\tTrain Loss: 0.247300\n",
      "Train Epoch: 22 [12800/60000(21%)]\tTrain Loss: 0.459203\n",
      "Train Epoch: 22 [19200/60000(32%)]\tTrain Loss: 0.229349\n",
      "Train Epoch: 22 [25600/60000(43%)]\tTrain Loss: 0.476838\n",
      "Train Epoch: 22 [32000/60000(53%)]\tTrain Loss: 0.235607\n",
      "Train Epoch: 22 [38400/60000(64%)]\tTrain Loss: 0.653781\n",
      "Train Epoch: 22 [44800/60000(75%)]\tTrain Loss: 0.598333\n",
      "Train Epoch: 22 [51200/60000(85%)]\tTrain Loss: 0.325154\n",
      "Train Epoch: 22 [57600/60000(96%)]\tTrain Loss: 0.549215\n",
      "\n",
      "[EPOCH:22], \tTest Loss: 0.0097, \tTest Accuracy: 90.69 %\n",
      "\n",
      "Train Epoch: 23 [0/60000(0%)]\tTrain Loss: 0.217987\n",
      "Train Epoch: 23 [6400/60000(11%)]\tTrain Loss: 0.425859\n",
      "Train Epoch: 23 [12800/60000(21%)]\tTrain Loss: 0.380102\n",
      "Train Epoch: 23 [19200/60000(32%)]\tTrain Loss: 0.424084\n",
      "Train Epoch: 23 [25600/60000(43%)]\tTrain Loss: 0.325220\n",
      "Train Epoch: 23 [32000/60000(53%)]\tTrain Loss: 0.176290\n",
      "Train Epoch: 23 [38400/60000(64%)]\tTrain Loss: 0.404464\n",
      "Train Epoch: 23 [44800/60000(75%)]\tTrain Loss: 0.329643\n",
      "Train Epoch: 23 [51200/60000(85%)]\tTrain Loss: 0.455095\n",
      "Train Epoch: 23 [57600/60000(96%)]\tTrain Loss: 0.607160\n",
      "\n",
      "[EPOCH:23], \tTest Loss: 0.0095, \tTest Accuracy: 90.72 %\n",
      "\n",
      "Train Epoch: 24 [0/60000(0%)]\tTrain Loss: 0.465066\n",
      "Train Epoch: 24 [6400/60000(11%)]\tTrain Loss: 0.280619\n",
      "Train Epoch: 24 [12800/60000(21%)]\tTrain Loss: 0.675213\n",
      "Train Epoch: 24 [19200/60000(32%)]\tTrain Loss: 0.216769\n",
      "Train Epoch: 24 [25600/60000(43%)]\tTrain Loss: 0.654542\n",
      "Train Epoch: 24 [32000/60000(53%)]\tTrain Loss: 0.280198\n",
      "Train Epoch: 24 [38400/60000(64%)]\tTrain Loss: 0.412013\n",
      "Train Epoch: 24 [44800/60000(75%)]\tTrain Loss: 0.165296\n",
      "Train Epoch: 24 [51200/60000(85%)]\tTrain Loss: 0.172530\n",
      "Train Epoch: 24 [57600/60000(96%)]\tTrain Loss: 0.525491\n",
      "\n",
      "[EPOCH:24], \tTest Loss: 0.0093, \tTest Accuracy: 90.93 %\n",
      "\n",
      "Train Epoch: 25 [0/60000(0%)]\tTrain Loss: 0.514389\n",
      "Train Epoch: 25 [6400/60000(11%)]\tTrain Loss: 0.428971\n",
      "Train Epoch: 25 [12800/60000(21%)]\tTrain Loss: 0.727393\n",
      "Train Epoch: 25 [19200/60000(32%)]\tTrain Loss: 0.428317\n",
      "Train Epoch: 25 [25600/60000(43%)]\tTrain Loss: 0.760692\n",
      "Train Epoch: 25 [32000/60000(53%)]\tTrain Loss: 0.542299\n",
      "Train Epoch: 25 [38400/60000(64%)]\tTrain Loss: 0.140888\n",
      "Train Epoch: 25 [44800/60000(75%)]\tTrain Loss: 0.568211\n",
      "Train Epoch: 25 [51200/60000(85%)]\tTrain Loss: 0.364380\n",
      "Train Epoch: 25 [57600/60000(96%)]\tTrain Loss: 0.359049\n",
      "\n",
      "[EPOCH:25], \tTest Loss: 0.0091, \tTest Accuracy: 91.15 %\n",
      "\n",
      "Train Epoch: 26 [0/60000(0%)]\tTrain Loss: 0.474031\n",
      "Train Epoch: 26 [6400/60000(11%)]\tTrain Loss: 0.650733\n",
      "Train Epoch: 26 [12800/60000(21%)]\tTrain Loss: 0.400682\n",
      "Train Epoch: 26 [19200/60000(32%)]\tTrain Loss: 0.412215\n",
      "Train Epoch: 26 [25600/60000(43%)]\tTrain Loss: 0.182346\n",
      "Train Epoch: 26 [32000/60000(53%)]\tTrain Loss: 0.208875\n",
      "Train Epoch: 26 [38400/60000(64%)]\tTrain Loss: 0.314926\n",
      "Train Epoch: 26 [44800/60000(75%)]\tTrain Loss: 0.322175\n",
      "Train Epoch: 26 [51200/60000(85%)]\tTrain Loss: 0.326521\n",
      "Train Epoch: 26 [57600/60000(96%)]\tTrain Loss: 0.245790\n",
      "\n",
      "[EPOCH:26], \tTest Loss: 0.0090, \tTest Accuracy: 91.37 %\n",
      "\n",
      "Train Epoch: 27 [0/60000(0%)]\tTrain Loss: 0.528059\n",
      "Train Epoch: 27 [6400/60000(11%)]\tTrain Loss: 0.281025\n",
      "Train Epoch: 27 [12800/60000(21%)]\tTrain Loss: 0.241644\n",
      "Train Epoch: 27 [19200/60000(32%)]\tTrain Loss: 0.484636\n",
      "Train Epoch: 27 [25600/60000(43%)]\tTrain Loss: 0.282091\n",
      "Train Epoch: 27 [32000/60000(53%)]\tTrain Loss: 0.520640\n",
      "Train Epoch: 27 [38400/60000(64%)]\tTrain Loss: 0.262349\n",
      "Train Epoch: 27 [44800/60000(75%)]\tTrain Loss: 0.513030\n",
      "Train Epoch: 27 [51200/60000(85%)]\tTrain Loss: 0.160578\n",
      "Train Epoch: 27 [57600/60000(96%)]\tTrain Loss: 0.484140\n",
      "\n",
      "[EPOCH:27], \tTest Loss: 0.0089, \tTest Accuracy: 91.31 %\n",
      "\n",
      "Train Epoch: 28 [0/60000(0%)]\tTrain Loss: 0.280971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [6400/60000(11%)]\tTrain Loss: 0.352260\n",
      "Train Epoch: 28 [12800/60000(21%)]\tTrain Loss: 0.604008\n",
      "Train Epoch: 28 [19200/60000(32%)]\tTrain Loss: 0.327361\n",
      "Train Epoch: 28 [25600/60000(43%)]\tTrain Loss: 0.490036\n",
      "Train Epoch: 28 [32000/60000(53%)]\tTrain Loss: 0.312949\n",
      "Train Epoch: 28 [38400/60000(64%)]\tTrain Loss: 0.439034\n",
      "Train Epoch: 28 [44800/60000(75%)]\tTrain Loss: 0.441333\n",
      "Train Epoch: 28 [51200/60000(85%)]\tTrain Loss: 0.322650\n",
      "Train Epoch: 28 [57600/60000(96%)]\tTrain Loss: 0.276841\n",
      "\n",
      "[EPOCH:28], \tTest Loss: 0.0087, \tTest Accuracy: 91.54 %\n",
      "\n",
      "Train Epoch: 29 [0/60000(0%)]\tTrain Loss: 0.451981\n",
      "Train Epoch: 29 [6400/60000(11%)]\tTrain Loss: 0.240538\n",
      "Train Epoch: 29 [12800/60000(21%)]\tTrain Loss: 0.269989\n",
      "Train Epoch: 29 [19200/60000(32%)]\tTrain Loss: 0.590693\n",
      "Train Epoch: 29 [25600/60000(43%)]\tTrain Loss: 0.609357\n",
      "Train Epoch: 29 [32000/60000(53%)]\tTrain Loss: 0.262110\n",
      "Train Epoch: 29 [38400/60000(64%)]\tTrain Loss: 0.373997\n",
      "Train Epoch: 29 [44800/60000(75%)]\tTrain Loss: 0.327814\n",
      "Train Epoch: 29 [51200/60000(85%)]\tTrain Loss: 0.333131\n",
      "Train Epoch: 29 [57600/60000(96%)]\tTrain Loss: 0.533180\n",
      "\n",
      "[EPOCH:29], \tTest Loss: 0.0086, \tTest Accuracy: 91.78 %\n",
      "\n",
      "Train Epoch: 30 [0/60000(0%)]\tTrain Loss: 0.543797\n",
      "Train Epoch: 30 [6400/60000(11%)]\tTrain Loss: 0.407604\n",
      "Train Epoch: 30 [12800/60000(21%)]\tTrain Loss: 0.582796\n",
      "Train Epoch: 30 [19200/60000(32%)]\tTrain Loss: 0.533300\n",
      "Train Epoch: 30 [25600/60000(43%)]\tTrain Loss: 0.242425\n",
      "Train Epoch: 30 [32000/60000(53%)]\tTrain Loss: 0.882798\n",
      "Train Epoch: 30 [38400/60000(64%)]\tTrain Loss: 0.487336\n",
      "Train Epoch: 30 [44800/60000(75%)]\tTrain Loss: 0.360834\n",
      "Train Epoch: 30 [51200/60000(85%)]\tTrain Loss: 0.392356\n",
      "Train Epoch: 30 [57600/60000(96%)]\tTrain Loss: 0.374080\n",
      "\n",
      "[EPOCH:30], \tTest Loss: 0.0085, \tTest Accuracy: 91.84 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP학습을 실행하면서 trian, test set의 loss 및 test set accuracy를 확인'''\n",
    "for Epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH:{}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(\n",
    "    Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd84d23",
   "metadata": {},
   "source": [
    "## 3-2.epochs=10, relu+dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b040211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.318203\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 2.038199\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 1.214384\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.598691\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.627240\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.372438\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.457532\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.731041\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.497152\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.412464\n",
      "\n",
      "[EPOCH:1], \tTest Loss: 0.0100, \tTest Accuracy: 91.01 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.402511\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.300515\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.510545\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.244224\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.399617\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.340749\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.320061\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.345449\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.194200\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.429085\n",
      "\n",
      "[EPOCH:2], \tTest Loss: 0.0068, \tTest Accuracy: 93.55 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.509654\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.379380\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.094257\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.242672\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.206284\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.341656\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.403774\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.331060\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.131495\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.341083\n",
      "\n",
      "[EPOCH:3], \tTest Loss: 0.0054, \tTest Accuracy: 94.73 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.185042\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.217331\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.143807\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.392933\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.100223\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.193807\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.075411\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.317619\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.172166\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.102497\n",
      "\n",
      "[EPOCH:4], \tTest Loss: 0.0045, \tTest Accuracy: 95.60 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.101739\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.160386\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.102060\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.132101\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.179499\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.133506\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.111789\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.057861\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.154786\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.345572\n",
      "\n",
      "[EPOCH:5], \tTest Loss: 0.0038, \tTest Accuracy: 96.39 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.094276\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.112742\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.097891\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.238686\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.342370\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.039594\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.211554\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.079759\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.175843\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.110168\n",
      "\n",
      "[EPOCH:6], \tTest Loss: 0.0034, \tTest Accuracy: 96.53 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.244561\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.033950\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.021772\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.126725\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.106062\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.212648\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.138978\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.063061\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.148224\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.230574\n",
      "\n",
      "[EPOCH:7], \tTest Loss: 0.0032, \tTest Accuracy: 96.81 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.263469\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.376034\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.026618\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.079360\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.201475\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.073140\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.034645\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.023464\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.106358\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.242935\n",
      "\n",
      "[EPOCH:8], \tTest Loss: 0.0030, \tTest Accuracy: 96.93 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.306956\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.283621\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.034058\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.134657\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.028390\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.161976\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.068460\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.071945\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.017825\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.114819\n",
      "\n",
      "[EPOCH:9], \tTest Loss: 0.0027, \tTest Accuracy: 97.24 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.121917\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.030477\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.128360\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.042339\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.235143\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.104750\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.251916\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.448453\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.042549\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.228255\n",
      "\n",
      "[EPOCH:10], \tTest Loss: 0.0026, \tTest Accuracy: 97.47 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP학습을 실행하면서 trian, test set의 loss 및 test set accuracy를 확인'''\n",
    "for Epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH:{}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(\n",
    "    Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac46c4",
   "metadata": {},
   "source": [
    "## 3-3. epochs=10, Dropout + ReLU + Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b5b799e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.491906\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 0.480169\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 0.403541\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.488333\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.536266\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.348192\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.124434\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.300207\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.304999\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.119229\n",
      "\n",
      "[EPOCH:1], \tTest Loss: 0.0048, \tTest Accuracy: 95.48 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.291372\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.243482\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.301401\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.075179\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.081466\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.044149\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.302465\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.224359\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.423380\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.356314\n",
      "\n",
      "[EPOCH:2], \tTest Loss: 0.0036, \tTest Accuracy: 96.59 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.482738\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.307524\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.541908\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.298339\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.412528\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.284869\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.168699\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.120786\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.346258\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.226520\n",
      "\n",
      "[EPOCH:3], \tTest Loss: 0.0030, \tTest Accuracy: 97.04 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.136119\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.075588\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.121814\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.337927\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.045700\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.088731\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.016331\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.312728\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.034682\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.134374\n",
      "\n",
      "[EPOCH:4], \tTest Loss: 0.0027, \tTest Accuracy: 97.29 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.230738\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.016966\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.115202\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.055785\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.127352\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.562914\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.168576\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.038836\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.105906\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.203605\n",
      "\n",
      "[EPOCH:5], \tTest Loss: 0.0025, \tTest Accuracy: 97.61 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.104264\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.250690\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.174694\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.383171\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.253871\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.212229\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.390181\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.131312\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.030967\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.035503\n",
      "\n",
      "[EPOCH:6], \tTest Loss: 0.0022, \tTest Accuracy: 97.77 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.229478\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.293601\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.125385\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.235712\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.011723\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.275881\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.100310\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.376188\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.344148\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.081858\n",
      "\n",
      "[EPOCH:7], \tTest Loss: 0.0022, \tTest Accuracy: 97.76 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.059078\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.071224\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.063296\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.174552\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.724162\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.056783\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.172690\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.197787\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.024483\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.033188\n",
      "\n",
      "[EPOCH:8], \tTest Loss: 0.0021, \tTest Accuracy: 97.87 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.066267\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.105031\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.310134\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.153179\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.065716\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.124389\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.137526\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.235066\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.158475\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.039725\n",
      "\n",
      "[EPOCH:9], \tTest Loss: 0.0021, \tTest Accuracy: 97.97 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.515786\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.047793\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.070143\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.125448\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.018859\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.187355\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.045597\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.314842\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.143294\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.044381\n",
      "\n",
      "[EPOCH:10], \tTest Loss: 0.0020, \tTest Accuracy: 98.11 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP학습을 실행하면서 trian, test set의 loss 및 test set accuracy를 확인'''\n",
    "for Epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH:{}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(\n",
    "    Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c40372",
   "metadata": {},
   "source": [
    "## 3-4. Dropout + ReLU + Batch normalization + He Uniform Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0589e3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 3.330101\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 0.668033\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 0.675462\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.448291\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.764537\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.451630\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.237209\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.436398\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.362564\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.568494\n",
      "\n",
      "[EPOCH:1], \tTest Loss: 0.0070, \tTest Accuracy: 93.39 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.595237\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.298523\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.498785\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.215108\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.321864\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.327670\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.271319\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.263732\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.230503\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.362412\n",
      "\n",
      "[EPOCH:2], \tTest Loss: 0.0054, \tTest Accuracy: 94.78 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.269726\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.181601\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.058392\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.499088\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.524878\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.243576\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.615779\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.475871\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.264088\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.201804\n",
      "\n",
      "[EPOCH:3], \tTest Loss: 0.0046, \tTest Accuracy: 95.54 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.472783\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.297468\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.286814\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.185668\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.267457\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.322450\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.487567\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.203534\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.244794\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.175198\n",
      "\n",
      "[EPOCH:4], \tTest Loss: 0.0041, \tTest Accuracy: 95.99 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.195635\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.113268\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.410634\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.432278\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.332722\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.351728\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.095695\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.126562\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.119920\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.456202\n",
      "\n",
      "[EPOCH:5], \tTest Loss: 0.0038, \tTest Accuracy: 96.12 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.284378\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.056156\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.092855\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.411786\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.239341\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.178504\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.128357\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.178392\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.120704\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.080695\n",
      "\n",
      "[EPOCH:6], \tTest Loss: 0.0035, \tTest Accuracy: 96.73 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.394057\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.160410\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.090017\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.378605\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.447957\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.305532\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.097502\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.392905\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.220598\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.247368\n",
      "\n",
      "[EPOCH:7], \tTest Loss: 0.0033, \tTest Accuracy: 96.83 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.154237\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.353733\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.057703\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.324933\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.148035\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.245205\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.195539\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.209302\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.110135\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.224529\n",
      "\n",
      "[EPOCH:8], \tTest Loss: 0.0031, \tTest Accuracy: 96.95 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.037222\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.113355\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.204975\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.074089\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.206078\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.543843\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.482009\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.230475\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.334950\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.179663\n",
      "\n",
      "[EPOCH:9], \tTest Loss: 0.0029, \tTest Accuracy: 97.18 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.179015\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.250219\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.164486\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.276804\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.114372\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.224323\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.068583\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.174976\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.127480\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.035269\n",
      "\n",
      "[EPOCH:10], \tTest Loss: 0.0028, \tTest Accuracy: 97.25 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP학습을 실행하면서 trian, test set의 loss 및 test set accuracy를 확인'''\n",
    "for Epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH:{}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(\n",
    "    Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fc15f",
   "metadata": {},
   "source": [
    "## 3-5. Dropout + ReLU + Batch normalization + He Uniform Initialization + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "884f6498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 3.314788\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 0.183814\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 0.214900\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.260503\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.644800\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.249400\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.475859\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.222512\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.341864\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.271582\n",
      "\n",
      "[EPOCH:1], \tTest Loss: 0.0041, \tTest Accuracy: 96.05 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.112106\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.505707\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.093931\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.711520\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.179438\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.280567\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.236296\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.237715\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.672907\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.063954\n",
      "\n",
      "[EPOCH:2], \tTest Loss: 0.0032, \tTest Accuracy: 96.75 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.162784\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.589663\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.095736\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.152527\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.167912\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.194205\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.449116\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.145741\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.079369\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.079158\n",
      "\n",
      "[EPOCH:3], \tTest Loss: 0.0029, \tTest Accuracy: 97.08 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.083127\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.335498\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.147583\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.199106\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.156531\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.203253\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.168313\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.569426\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.126953\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.261377\n",
      "\n",
      "[EPOCH:4], \tTest Loss: 0.0028, \tTest Accuracy: 97.37 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.170820\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.351862\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.091407\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.112545\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.146017\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.039653\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.038479\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.061776\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.093190\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.087126\n",
      "\n",
      "[EPOCH:5], \tTest Loss: 0.0026, \tTest Accuracy: 97.54 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.110824\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.088500\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.171214\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.094485\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.086695\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.152213\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.088056\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.524374\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.273362\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.066271\n",
      "\n",
      "[EPOCH:6], \tTest Loss: 0.0024, \tTest Accuracy: 97.52 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.079228\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.157396\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.078319\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.267246\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.061389\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.337437\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.091883\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.107649\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.304659\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.214131\n",
      "\n",
      "[EPOCH:7], \tTest Loss: 0.0024, \tTest Accuracy: 97.68 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.033645\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.114572\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.142299\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.180569\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.204563\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.267734\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.190207\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.276364\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.102901\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.190484\n",
      "\n",
      "[EPOCH:8], \tTest Loss: 0.0023, \tTest Accuracy: 97.66 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.213855\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.219849\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.565380\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.245788\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.061778\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.022174\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.093155\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.314634\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.045994\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.134377\n",
      "\n",
      "[EPOCH:9], \tTest Loss: 0.0020, \tTest Accuracy: 97.94 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.045955\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.318005\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.076612\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.368136\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.016077\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.085124\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.119211\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.034424\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.011621\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.063131\n",
      "\n",
      "[EPOCH:10], \tTest Loss: 0.0021, \tTest Accuracy: 97.85 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP학습을 실행하면서 trian, test set의 loss 및 test set accuracy를 확인'''\n",
    "for Epoch in range(1, EPOCHS+1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH:{}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(\n",
    "    Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b306756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca246457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice[py-3.7]",
   "language": "python",
   "name": "practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
