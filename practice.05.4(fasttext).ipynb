{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d56d98c",
   "metadata": {},
   "source": [
    "## 5-4. 데이터 전처리 및 Pre-Trained Embedding Vector를 이용한 Vocabulary생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1492a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6563305",
   "metadata": {},
   "source": [
    "현재 내 python=3.7, pytorch version=1.4.0과 맞는 torchtext는 0.5.0 \n",
    "but, 실습하기 위해서는 torchtext==0.3.1 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70ba8c",
   "metadata": {},
   "source": [
    "pip install torchtext==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a89a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289ec3d",
   "metadata": {},
   "source": [
    "## Reading Data(IMDb data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbf565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setting\n",
    "TEXT = data.Field(batch_first = True, #Batch Size를 Data Shape Axis의 가장 앞으로 설정하는 옵션\n",
    "                  fix_length = 500, #sentence의 길이를 미리 제한\n",
    "                  tokenize=str.split, #tokenize를 설정하는 옵션. 기본값은 띄어쓰기 기반의 파이썬의 string.split함수\n",
    "                  pad_first=True,#fix_length대비 짧은 문장의 경우 padding을 해야 하는데 padding을 앞에서 줄 것인지에 대한 옵션\n",
    "                  pad_token='[PAD]',#padding에 대한 특수 token설정\n",
    "                  unk_token='[UNK]')#token dict에 없는 token이 나왔을 경우 해당 token을 표현하는 특수 token\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.float)#가져올 데이터에 대한 type설정 옵션\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(text_field = TEXT, \n",
    "                                             label_field = LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ef1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length : 25000\n",
      "Test Data Length : 25000\n"
     ]
    }
   ],
   "source": [
    "# Data Length\n",
    "print(f'Train Data Length : {len(train_data.examples)}') #data.examples : 데이터의 개수 확인\n",
    "print(f'Test Data Length : {len(test_data.examples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda1dc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x25ab6061748>,\n",
       " 'label': <torchtext.data.field.LabelField at 0x25ab6061708>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Fields\n",
    "train_data.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9510c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Data Sample ----\n",
      "Input : \n",
      "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others. \n",
      "\n",
      "Label : \n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# Data Sample\n",
    "print('---- Data Sample ----')\n",
    "print('Input : ')\n",
    "print(' '.join(vars(train_data.examples[1])['text']),'\\n') #vars():데이터 값을 직접 확인\n",
    "print('Label : ')\n",
    "print(vars(train_data.examples[0])['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb171ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.<br /><br />There's not a single good line or character in the whole mess. If there was a plot, it was an afterthought and as far as acting goes, there's nothing good to say so Ill say nothing. I honestly cant understand how this type of nonsense gets produced and actually released, does somebody somewhere not at some stage think, 'Oh my god this really is a load of shite' and call it a day. Its crap like this that has people downloading illegally, the trailer looks like a completely different film, at least if you have download it, you haven't wasted your time or money Don't waste your time, this is painful. \n",
      "\n",
      "David Bryce's comments nearby are exceptionally well written and informative as almost say everything I feel about DARLING LILI. This massive musical is so peculiar and over blown, over produced and must have caused ruptures at Paramount in 1970. It cost 22 million dollars! That is simply irresponsible. DARLING LILI must have been greenlit from a board meeting that said \"hey we got that Pink Panther guy and that Sound Of Music gal... lets get this too\" and handed over a blank cheque. The result is a hybrid of GIGI, ZEPPELIN, HALF A SIXPENCE, some MGM 40s song and dance numbers of a style (daisies and boaters!) so hopelessly old fashioned as to be like musical porridge, and MATA HARI dramatics. The production is colossal, lush, breathtaking to view, but the rest: the ridiculous romance, Julie looking befuddled, Hudson already dead, the mistimed comedy, and the astoundingly boring songs deaden this spectacular film into being irritating. LILI is like a twee 1940s mega musical with some vulgar bits to spice it up. STAR! released the year before sadly crashed and now is being finally appreciated for the excellent film is genuinely is... and Andrews looks sublime, mature, especially in the last half hour......but LILI is POPPINS and DOLLY frilly and I believe really killed off the mega musical binge of the 60s..... and made Andrews look like Poppins again... which I believe was not Edwards intention. Paramount must have collectively fainted when they saw this: and with another $20 million festering in CATCH 22, and $12 million in ON A CLEAR DAY and $25 million in PAINT YOUR WAGON....they had a financial abyss of CLEOPATRA proportions with $77 million tied into 4 films with very uncertain futures. Maybe they should have asked seer Daisy Gamble from ON A CLEAR DAY ......LILI was very popular on immediate first release in Australia and ran in 70mm cinemas for months but it failed once out in the subs and the sticks and only ever surfaced after that on one night stands with ON A CLEAR DAY as a Sunday night double. Thank god Paramount had their simple $1million (yes, ONE MILLION DOLLAR) film LOVE STORY and that $4 million dollar gangster pic THE GODFATHER also ready to recover all the $77 million in just the next two years....for just $5m.... incredible! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(vars(train_data.examples[24999])['text']),'\\n')\n",
    "print(' '.join(vars(test_data.examples[24999])['text']),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e91b5",
   "metadata": {},
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93501dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#전처리 과정-특수문자, 소문자화, <br>텍스트\n",
    "def PreProcessingText(input_sentence):\n",
    "    input_sentence = input_sentence.lower() # 소문자화\n",
    "    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) #'<[^>]*>'부분을 \"<br />\"로 처리\n",
    "    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence)#특수문자 처리 (\"'\" 제외)\n",
    "    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리, \\s+:하나 이상의 공백 문자 시퀀스\n",
    "    if input_sentence:\n",
    "        return input_sentence\n",
    "    \n",
    "for example in train_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n",
    "\n",
    "for example in test_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1200d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david', \"bryce's\", 'comments', 'nearby', 'are', 'exceptionally', 'well', 'written', 'and', 'informative', 'as', 'almost', 'say', 'everything', 'i', 'feel', 'about', 'darling', 'lili', 'this', 'massive', 'musical', 'is', 'so', 'peculiar', 'and', 'over', 'blown', 'over', 'produced', 'and', 'must', 'have', 'caused', 'ruptures', 'at', 'paramount', 'in', '1970', 'it', 'cost', '22', 'million', 'dollars', 'that', 'is', 'simply', 'irresponsible', 'darling', 'lili', 'must', 'have', 'been', 'greenlit', 'from', 'a', 'board', 'meeting', 'that', 'said', 'hey', 'we', 'got', 'that', 'pink', 'panther', 'guy', 'and', 'that', 'sound', 'of', 'music', 'gal', 'lets', 'get', 'this', 'too', 'and', 'handed', 'over', 'a', 'blank', 'cheque', 'the', 'result', 'is', 'a', 'hybrid', 'of', 'gigi', 'zeppelin', 'half', 'a', 'sixpence', 'some', 'mgm', '40s', 'song', 'and', 'dance', 'numbers', 'of', 'a', 'style', 'daisies', 'and', 'boaters', 'so', 'hopelessly', 'old', 'fashioned', 'as', 'to', 'be', 'like', 'musical', 'porridge', 'and', 'mata', 'hari', 'dramatics', 'the', 'production', 'is', 'colossal', 'lush', 'breathtaking', 'to', 'view', 'but', 'the', 'rest', 'the', 'ridiculous', 'romance', 'julie', 'looking', 'befuddled', 'hudson', 'already', 'dead', 'the', 'mistimed', 'comedy', 'and', 'the', 'astoundingly', 'boring', 'songs', 'deaden', 'this', 'spectacular', 'film', 'into', 'being', 'irritating', 'lili', 'is', 'like', 'a', 'twee', '1940s', 'mega', 'musical', 'with', 'some', 'vulgar', 'bits', 'to', 'spice', 'it', 'up', 'star', 'released', 'the', 'year', 'before', 'sadly', 'crashed', 'and', 'now', 'is', 'being', 'finally', 'appreciated', 'for', 'the', 'excellent', 'film', 'is', 'genuinely', 'is', 'and', 'andrews', 'looks', 'sublime', 'mature', 'especially', 'in', 'the', 'last', 'half', 'hour', 'but', 'lili', 'is', 'poppins', 'and', 'dolly', 'frilly', 'and', 'i', 'believe', 'really', 'killed', 'off', 'the', 'mega', 'musical', 'binge', 'of', 'the', '60s', 'and', 'made', 'andrews', 'look', 'like', 'poppins', 'again', 'which', 'i', 'believe', 'was', 'not', 'edwards', 'intention', 'paramount', 'must', 'have', 'collectively', 'fainted', 'when', 'they', 'saw', 'this', 'and', 'with', 'another', '20', 'million', 'festering', 'in', 'catch', '22', 'and', '12', 'million', 'in', 'on', 'a', 'clear', 'day', 'and', '25', 'million', 'in', 'paint', 'your', 'wagon', 'they', 'had', 'a', 'financial', 'abyss', 'of', 'cleopatra', 'proportions', 'with', '77', 'million', 'tied', 'into', '4', 'films', 'with', 'very', 'uncertain', 'futures', 'maybe', 'they', 'should', 'have', 'asked', 'seer', 'daisy', 'gamble', 'from', 'on', 'a', 'clear', 'day', 'lili', 'was', 'very', 'popular', 'on', 'immediate', 'first', 'release', 'in', 'australia', 'and', 'ran', 'in', '70mm', 'cinemas', 'for', 'months', 'but', 'it', 'failed', 'once', 'out', 'in', 'the', 'subs', 'and', 'the', 'sticks', 'and', 'only', 'ever', 'surfaced', 'after', 'that', 'on', 'one', 'night', 'stands', 'with', 'on', 'a', 'clear', 'day', 'as', 'a', 'sunday', 'night', 'double', 'thank', 'god', 'paramount', 'had', 'their', 'simple', '1million', 'yes', 'one', 'million', 'dollar', 'film', 'love', 'story', 'and', 'that', '4', 'million', 'dollar', 'gangster', 'pic', 'the', 'godfather', 'also', 'ready', 'to', 'recover', 'all', 'the', '77', 'million', 'in', 'just', 'the', 'next', 'two', 'years', 'for', 'just', '5m', 'incredible']\n"
     ]
    }
   ],
   "source": [
    "print(vars(example)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f2ee6",
   "metadata": {},
   "source": [
    "David Bryce's comments nearby are exceptionally well written and informative as almost say everything I feel about DARLING LILI.\n",
    "\n",
    "=>'david', \"bryce's\", 'comments', 'nearby', 'are', 'exceptionally', 'well', 'written', 'and', 'informative', 'as', 'almost', 'say', 'everything', 'i', 'feel', 'about', 'darling', 'lili'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425d7f7",
   "metadata": {},
   "source": [
    "## making vocab & setting embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9190ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'emb_type' : 'fasttext', 'emb_dim' : 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e082c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext pre-trained \n",
    "#미리 작업해둔 Field에 build_vocab을 이용해 text data와 label data의 vocab을 손쉽게 만듦\n",
    "'''TEXT.build_vocab(train_data,\n",
    "                 min_freq = 2,#vocab에 해당하는 token에 최소한으로 등장하는 횟수 제한\n",
    "                 max_size = None,#전체 vocab size제한\n",
    "                 #vectors = f\"glove.6B.{model_config['emb_dim']}d\"\n",
    "                 #pre-trained vector를 가져와 vocab에 세팅. 원하는 임베딩을 정해 string형태로 설정\n",
    "                )\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "model_config['vocab_size'] = len(TEXT.vocab)'''\n",
    "\n",
    "#! wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6edb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext pre-trained \n",
    "from torchtext.vocab import Vectors\n",
    "fasttext_vectors = Vectors('./wiki.en.vec')\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 min_freq = 2, \n",
    "                 max_size = None,\n",
    "                 vectors = fasttext_vectors)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "model_config['vocab_size'] = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e15110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size : 51956\n",
      "Vocab Examples : \n",
      "\t [UNK] 0\n",
      "\t [PAD] 1\n",
      "\t the 2\n",
      "\t and 3\n",
      "\t a 4\n",
      "\t of 5\n",
      "\t to 6\n",
      "\t is 7\n",
      "\t in 8\n",
      "\t it 9\n",
      "---------------------------------\n",
      "Label Size : 2\n",
      "Lable Examples : \n",
      "\t neg 0\n",
      "\t pos 1\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Info\n",
    "print(f'Vocab Size : {len(TEXT.vocab)}')\n",
    "\n",
    "print('Vocab Examples : ')\n",
    "for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n",
    "    if idx >= 10:\n",
    "        break    \n",
    "    print('\\t', k, v)\n",
    "\n",
    "print('---------------------------------')\n",
    "\n",
    "# Label Info\n",
    "print(f'Label Size : {len(LABEL.vocab)}')\n",
    "\n",
    "print('Lable Examples : ')\n",
    "for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n",
    "    print('\\t', k, v)\n",
    "    \n",
    "# Check embedding vectors\n",
    "#TEXT.vocab.vectors.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b6a7a",
   "metadata": {},
   "source": [
    "## spliting validation data & making data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfb12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Spliting Valid set\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(0),\n",
    "                                          split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea1268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['batch_size'] = 30\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#BucketIterator를 이용해 쉽게 batch data 생성\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data), \n",
    "                                                                           batch_size=model_config['batch_size'],\n",
    "                                                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a163294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 30]\n",
      "\t[.text]:[torch.LongTensor of size 30x500]\n",
      "\t[.label]:[torch.FloatTensor of size 30]\n",
      "tensor([[    1,     1,     1,  ...,  1262,    22,   119],\n",
      "        [    1,     1,     1,  ...,  5769,     3,  4838],\n",
      "        [    1,     1,     1,  ...,  3035,    76,  4462],\n",
      "        ...,\n",
      "        [    1,     1,     1,  ..., 15469,  5258,     0],\n",
      "        [    1,     1,     1,  ...,    16,   916,   467],\n",
      "        [    1,     1,     1,  ...,    24,   233,  2630]])\n",
      "tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Check batch data\n",
    "sample_for_check = next(iter(train_iterator))\n",
    "print(sample_for_check)\n",
    "print(sample_for_check.text)\n",
    "print(sample_for_check.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982fa3e",
   "metadata": {},
   "source": [
    "## Pytorch에서 RNN, LSTM, GRU을 이용한 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ce9b6",
   "metadata": {},
   "source": [
    "## Making Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a391d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassification(nn.Module):\n",
    "    def __init__(self, **model_config):\n",
    "        super(SentenceClassification, self).__init__()\n",
    "\n",
    "        if model_config['emb_type'] == 'glove' or 'fasttext':#input:token index값을 가진 vector. vocab_size*embedding_dimension 행렬을 만들어 학습\n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],#num_embeddings: vocab size\n",
    "                                    embedding_dim = model_config['emb_dim'],#embedding_dim:원하는 embedding dim 설정\n",
    "                                    _weight = TEXT.vocab.vectors)#pre-trained vector를 embedding 행렬의 initial value로 설정. 이 옵션이 없을 경우 정규분포에서 생성한 값을 initial value로 설정하고 학습\n",
    "        else:\n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],\n",
    "                                    embedding_dim = model_config['emb_dim'])\n",
    "        \n",
    "        self.bidirectional = model_config['bidirectional']\n",
    "        self.num_direction = 2 if model_config['bidirectional'] else 1\n",
    "        self.model_type = model_config['model_type'] \n",
    "\n",
    "        self.RNN = nn.RNN(input_size = model_config['emb_dim'],#입력받을 data의 크기. embedding dimension설정\n",
    "                          hidden_size = model_config['hidden_dim'],#hidden layer의 dimension설정\n",
    "                          dropout = model_config['dropout'],\n",
    "                          bidirectional = model_config['bidirectional'],\n",
    "                          batch_first = model_config['batch_first'])#data의 제일 처음 axis에 batch_size가 오도록 설정\n",
    "        \n",
    "        self.LSTM= nn.LSTM(input_size = model_config['emb_dim'],\n",
    "                           hidden_size = model_config['hidden_dim'],\n",
    "                           dropout = model_config['dropout'],\n",
    "                           bidirectional = model_config['bidirectional'],\n",
    "                           batch_first = model_config['batch_first'])\n",
    "        \n",
    "        self.GRU = nn.GRU(input_size = model_config['emb_dim'],\n",
    "                          hidden_size = model_config['hidden_dim'],\n",
    "                          dropout = model_config['dropout'],\n",
    "                          bidirectional = model_config['bidirectional'],\n",
    "                          batch_first = model_config['batch_first'])\n",
    "    \n",
    "        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction,#분류문제를 푸는 task를 할 예정이므로 class에 대한 score를 생성하기 위해 fc layer를 1개 만들어 통과시킴.\n",
    "                            model_config['output_dim'])#추후 시그모이드가 없는 이유는 추후 loss function에 포함돼 있기 때문\n",
    "        \n",
    "        self.drop = nn.Dropout(model_config['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        emb = self.emb(x) \n",
    "        # emb : (Batch_Size, Max_Seq_Length, Emb_dim)\n",
    "\n",
    "        if self.model_type == 'RNN':\n",
    "            output, hidden = self.RNN(emb) \n",
    "        elif self.model_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.LSTM(emb)\n",
    "        elif self.model_type == 'GRU':\n",
    "            output, hidden = self.GRU(emb)\n",
    "        else:\n",
    "            raise NameError('Select model_type in [RNN, LSTM, GRU]')\n",
    "        \n",
    "        # output : (Batch_Size, Max_Seq_Length, Hidden_dim * num_direction) \n",
    "        # hidden : (num_direction, Batch_Size, Hidden_dim)\n",
    "        # hidden의 경우, batch_first 옵션이 안먹는 문제가 있음\n",
    "        \n",
    "        last_output = output[:,-1,:] #output shape은 token의 위치를 설명하는 두번째 차원에서 마지막 값을 가져와 사용\n",
    "\n",
    "        # last_output : (Batch_Size, Hidden_dim * num_direction)\n",
    "        return self.fc(self.drop(last_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1abad",
   "metadata": {},
   "source": [
    "## checking feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0788557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3307,  0.4489,  0.0787, -0.1998, -0.2811, -0.2645, -0.5108, -0.3324,\n",
      "        -0.2175,  0.4181, -0.2781,  0.0740, -0.4978,  0.7202, -0.2802, -0.5279,\n",
      "        -0.2119, -0.1287, -1.1439, -0.3933,  0.0530, -0.3471, -0.8656, -0.0407,\n",
      "         0.1841, -0.4621, -0.1746, -0.1644, -0.1857, -0.1422],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.7204, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.4667)\n"
     ]
    }
   ],
   "source": [
    "model_config.update(dict(batch_first = True,\n",
    "                         model_type = 'RNN',#RNN, LSTM, GRU 중 하나 선택\n",
    "                         bidirectional = True, #양방향 선택\n",
    "                         hidden_dim = 128,\n",
    "                         output_dim = 1, #IMDb분류는 binary classification이므로\n",
    "                         dropout = 0))\n",
    "\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "\n",
    "predictions = model.forward(sample_for_check.text).squeeze()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "loss = loss_fn(predictions, sample_for_check.label)\n",
    "acc = binary_accuracy(predictions, sample_for_check.label)\n",
    "\n",
    "print(predictions)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4b60e",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e0196a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_params):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() \n",
    "    batch_size = model_params['batch_size']\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "        \n",
    "        # Initializing\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward \n",
    "        predictions = model(batch.text).squeeze()\n",
    "        loss = loss_fn(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        sys.stdout.write(\n",
    "                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n",
    "                    f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size} ({100. * (idx + 1) / len(iterator) :.4}%)]\"\\\n",
    "                    f\"  Loss: {loss.item():.4}\"\\\n",
    "                    f\"  Acc : {acc.item():.4}\"\\\n",
    "                    )\n",
    "\n",
    "        # Backward \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update Epoch Performance\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "304c16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = loss_fn(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31c63b",
   "metadata": {},
   "source": [
    "## bi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5400e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'RNN'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30076b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Model name : bi-RNN_\n",
      "---------------------------------\n",
      "[Train] Epoch :  0 [20010 / 20010 (100.0%)]  Loss: 0.6795  Acc : 0.5333\n",
      "\t Saved at 0-epoch\n",
      "\t Epoch : 0 | Train Loss : 0.6333 | Train Acc : 0.6351\n",
      "\t Epoch : 0 | Valid Loss : 0.6239 | Valid Acc : 0.6356\n",
      "[Train] Epoch :  1 [20010 / 20010 (100.0%)]  Loss: 0.7517  Acc : 0.6333\n",
      "\t Epoch : 1 | Train Loss : 0.6192 | Train Acc : 0.6449\n",
      "\t Epoch : 1 | Valid Loss : 0.6446 | Valid Acc : 0.6201\n",
      "[Train] Epoch :  2 [20010 / 20010 (100.0%)]  Loss: 0.6333  Acc : 0.7333\n",
      "\t Epoch : 2 | Train Loss : 0.6027 | Train Acc : 0.6623\n",
      "\t Epoch : 2 | Valid Loss : 0.6562 | Valid Acc : 0.6108\n",
      "[Train] Epoch :  3 [14940 / 20010 (74.66%)]  Loss: 0.6388  Acc : 0.7677"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b351f0",
   "metadata": {},
   "source": [
    "## bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'LSTM'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8eca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e90beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a7f87",
   "metadata": {},
   "source": [
    "## bi-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'GRU'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4443678",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe453ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96535f",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ff24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'GRU'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "model.load_state_dict(torch.load(f\"./{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14515ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    indexed = TEXT.numericalize(TEXT.pad([TEXT.tokenize(PreProcessingText(sentence))]))\n",
    "    input_data = torch.LongTensor(indexed).to(device)\n",
    "    prediction = torch.sigmoid(model(input_data))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'this movie is FUN'\n",
    "predict_sentiment(model = model, sentence = test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb264a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e6c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab56b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ef293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac289b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice[py-3.7]",
   "language": "python",
   "name": "practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
