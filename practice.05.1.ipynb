{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4edf6acb",
   "metadata": {},
   "source": [
    "# NLP\n",
    "### Text 데이터를 분석하고 모델링하는 분야 : 자연어처리(NLP)\n",
    "#### 자연어를 이해하는 영역(NLU) + 모델이 자연어를 생성하는 영역(NLG) = NLP\n",
    "\n",
    "#### 다양한 Task\n",
    "1. 감정분석\n",
    "2. 요약\n",
    "3. 기계 번역\n",
    "4. 질문 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b37497",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = '나는 책상 위에 사과를 먹었다'       \n",
    "S2 = '알고 보니 그 사과는 Jason 것이었다' \n",
    "S3 = '그래서 Jason에게 사과를 했다'\n",
    "\n",
    "S4 = '나는 책상 위에 배를 먹었다'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a94e17",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbc1aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '책상', '위에', '사과를', '먹었다']\n",
      "['알고', '보니', '그', '사과는', 'Jason', '것이었다']\n",
      "['그래서', 'Jason에게', '사과를', '했다']\n"
     ]
    }
   ],
   "source": [
    "print(S1.split())\n",
    "print(S2.split())\n",
    "print(S3.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eafce7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', ' ', '책', '상', ' ', '위', '에', ' ', '사', '과', '를', ' ', '먹', '었', '다']\n"
     ]
    }
   ],
   "source": [
    "print(list(S1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb6d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 0, '책상': 1, '위에': 2, '사과를': 3, '먹었다': 4, '알고': 5, '보니': 6, '그': 7, '사과는': 8, 'Jason': 9, '것이었다': 10, '그래서': 11, 'Jason에게': 12, '했다': 13}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {}\n",
    "index = 0\n",
    "\n",
    "for sentence in [S1, S2, S3]:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token2idx.get(token) == None:\n",
    "            token2idx[token] = index\n",
    "            index += 1\n",
    "\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c3569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[5, 6, 7, 8, 9, 10]\n",
      "[11, 12, 3, 13]\n"
     ]
    }
   ],
   "source": [
    "def indexed_sentence(sentence):\n",
    "    return [token2idx[token] for token in sentence]\n",
    "    \n",
    "S1_i = indexed_sentence(S1.split())\n",
    "print(S1_i)\n",
    "\n",
    "S2_i = indexed_sentence(S2.split())\n",
    "print(S2_i)\n",
    "\n",
    "S3_i = indexed_sentence(S3.split())\n",
    "print(S3_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81ae1d",
   "metadata": {},
   "source": [
    "## OOV(out-of-vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9df4bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'배를'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5248\\231175917.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mS4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'나는 책상 위에 배를 먹었다'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mindexed_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# KeyError: '배를'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5248\\1512987958.py\u001b[0m in \u001b[0;36mindexed_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mindexed_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mS1_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexed_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS1_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5248\\1512987958.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mindexed_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mS1_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexed_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS1_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '배를'"
     ]
    }
   ],
   "source": [
    "S4 = '나는 책상 위에 배를 먹었다'\n",
    "\n",
    "indexed_sentence(S4.split())\n",
    "# KeyError: '배를'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1b2720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 0, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S4 = '나는 책상 위에 배를 먹었다'\n",
    "\n",
    "# 기존 token 사전에 <unk> token 추가\n",
    "token2idx = {t : i+1 for t, i in token2idx.items()}\n",
    "token2idx['<unk>'] = 0\n",
    "\n",
    "# token이 없을 경우, <unk> token의 0을 치환\n",
    "def indexed_sentence_unk(sentence):\n",
    "    return [token2idx.get(token, token2idx['<unk>']) for token in sentence]\n",
    "\n",
    "indexed_sentence_unk(S4.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa19d50",
   "metadata": {},
   "source": [
    "문장을 띄어쓰기 단위가 아닌 글자로 token사용 => OOV현상 방지  \n",
    "그러나 글자 하나에는 의미가 거의 없음.  \n",
    "짧은 문장이나 단어를 이용하는 task가 아니면 글자 기반 tokenizer는 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a228dc8",
   "metadata": {},
   "source": [
    "## n-gram\n",
    "여러 개(n)의 연속된 윈도우를 단위로 살펴보는 방법  \n",
    "n>=4이면 n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00e90dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', ' ', '책', '상', ' ', '위', '에', ' ', '사', '과', '를', ' ', '먹', '었', '다']\n",
      "['나는', '는 ', ' 책', '책상', '상 ', ' 위', '위에', '에 ', ' 사', '사과', '과를', '를 ', ' 먹', '먹었', '었다', '다']\n",
      "['나는 ', '는 책', ' 책상', '책상 ', '상 위', ' 위에', '위에 ', '에 사', ' 사과', '사과를', '과를 ', '를 먹', ' 먹었', '먹었다', '었다', '다']\n"
     ]
    }
   ],
   "source": [
    "S1 = '나는 책상 위에 사과를 먹었다'\n",
    "\n",
    "print([S1[i:i+1] for i in range(len(S1))]) # uni-gram(n=1)\n",
    "print([S1[i:i+2] for i in range(len(S1))]) # bi -gram(n=2)\n",
    "print([S1[i:i+3] for i in range(len(S1))]) # tri-gramb(b=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501a6b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'dying', 'to', 'play', 'the', 'game']\n",
      "['I am', 'am dying', 'dying to', 'to play', 'play the', 'the game', 'game']\n",
      "['I am dying', 'am dying to', 'dying to play', 'to play the', 'play the game', 'the game', 'game']\n"
     ]
    }
   ],
   "source": [
    "S5 = 'I am dying to play the game'\n",
    "S5_sp = S5.split()\n",
    "\n",
    "print([\" \".join(S5_sp[i:i+1]) for i in range(len(S5_sp))]) # uni-gram\n",
    "print([\" \".join(S5_sp[i:i+2]) for i in range(len(S5_sp))]) # bi -gram\n",
    "print([\" \".join(S5_sp[i:i+3]) for i in range(len(S5_sp))]) # tri-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19374c",
   "metadata": {},
   "source": [
    "그러나 무의미한 조합들이 너무 많이 생김.  \n",
    "\n",
    "n-gram의 이점과 의미있는 것들만 token으로 사용하는 방법 => BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77321968",
   "metadata": {},
   "source": [
    "## BPE\n",
    "자주 나오는 글자의 나열에 치환을 이용하여 효율적인 token사용  \n",
    "1. 띄어쓰기 기반 tokenization\n",
    "2. 연속된 2개의 글자의 숫자를 세어 가장 많이 나오는 글자 2개의조합 찾기\n",
    "3. 두 글자를 합쳐 기존 사전의 단어 수정\n",
    "4. 미리 정해 놓은 횟수만큼 2~3번의 과정 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b24c1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "('e', 's')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "\n",
      "Step 2\n",
      "('es', 't')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "\n",
      "Step 3\n",
      "('est', '</w>')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      "Step 4\n",
      "('l', 'o')\n",
      "{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      "Step 5\n",
      "('lo', 'w')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      "Step 6\n",
      "('n', 'e')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      "Step 7\n",
      "('ne', 'w')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      "Step 8\n",
      "('new', 'est</w>')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      "Step 9\n",
      "('low', '</w>')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "\n",
      "Step 10\n",
      "('w', 'i')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Algorithm 1: Learn BPE operations\n",
    "import re, collections\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "            '''Step 1\n",
    "            {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, \n",
    "            ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, \n",
    "            ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, \n",
    "            ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, \n",
    "            ('i', 'd'): 3, ('d', 'e'): 3}'''\n",
    "    return pairs\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\\\S)'+bigram+r'(?!\\\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>':3} #1\n",
    "num_merges = 10\n",
    "for i in range(num_merges): #4\n",
    "    pairs = get_stats(vocab) #2\n",
    "    best = max(pairs, key=pairs.get) #2\n",
    "    vocab = merge_vocab(best, vocab) #3\n",
    "    print(f'Step {i+1}')\n",
    "    print(best)\n",
    "    print(vocab)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f6ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사 과 를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사 과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n"
     ]
    }
   ],
   "source": [
    "S1 = '나는 책상 위에 사과를 먹었다'       \n",
    "S2 = '알고 보니 그 사과는 Jason 것이었다' \n",
    "S3 = '그래서 Jason에게 사과를 했다'\n",
    "\n",
    "token_counts = {}\n",
    "index = 0\n",
    "\n",
    "for sentence in [S1, S2, S3]:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token_counts.get(token) == None:\n",
    "            token_counts[token] = 1\n",
    "        else:\n",
    "            token_counts[token] += 1\n",
    "\n",
    "token_counts = {\" \".join(token) : counts for token, counts in token_counts.items()}\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bcf6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "('사', '과')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과 를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 2\n",
      "('사과', '를')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 3\n",
      "('었', '다')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 4\n",
      "('J', 'a')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Ja s o n': 1, '것 이 었다': 1, '그 래 서': 1, 'Ja s o n 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 5\n",
      "('Ja', 's')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jas o n': 1, '것 이 었다': 1, '그 래 서': 1, 'Jas o n 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 6\n",
      "('Jas', 'o')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jaso n': 1, '것 이 었다': 1, '그 래 서': 1, 'Jaso n 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 7\n",
      "('Jaso', 'n')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 8\n",
      "('나', '는')\n",
      "{'나는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 9\n",
      "('책', '상')\n",
      "{'나는': 1, '책상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n",
      "Step 10\n",
      "('위', '에')\n",
      "{'나는': 1, '책상': 1, '위에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(token_counts)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    token_counts = merge_vocab(best, token_counts)\n",
    "    print(f'Step {i + 1}')\n",
    "    print(best)\n",
    "    print(token_counts)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22365a59",
   "metadata": {},
   "source": [
    "## 5-2. pre-trained tokenizer 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed21eabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "     -------------------------------------- 190.3/190.3 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from transformers) (6.0.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-win_amd64.whl (268 kB)\n",
      "     -------------------------------------- 268.0/268.0 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.2/153.2 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from importlib-metadata->transformers) (3.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, regex, pyyaml, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.9.0 huggingface-hub-0.12.1 pyyaml-6.0 regex-2022.10.31 tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f8fa7b2bed4f93a6a1d927e7a4e98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leeej\\anaconda3\\envs\\practice\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\leeej\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ebbf78659441f3baa2c1d51d972c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21388123a2044a4292bfd75e0a4ce708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "# 5-5_model_imdb_BERT.ipynb Code 확인\n",
    "!pip install transformers\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e5ad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cute', '.', 'he', 'likes', 'playing']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence = \"My dog is cute. He likes playing\"\n",
    "print(tokenizer.tokenize(sentence))\n",
    "#split과 다르지 않은 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dfd6212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105879\n",
      "['my', 'dog', 'is', 'cut', '##e', '.', 'he', 'likes', 'playing']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "print(len(tokenizer.vocab))\n",
    "print(tokenizer.tokenize(sentence))\n",
    "#학습한 데이터에 따라 tokenizer가 달라짐.\n",
    "#'##e'는 앞에 띄어쓰기가 아닌 바로 이어지는 token의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "448cbc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', 'ᄎ', '##ᅢᆨ', '##상', '위에', 'ᄉ', '##ᅡ', '##과', '##를', 'ᄆ', '##ᅥ', '##ᆨ', '##었다', '.', '알', '##고', 'ᄇ', '##ᅩ', '##니', '그', 'ᄉ', '##ᅡ', '##과', '##는', 'jason', '것이', '##었다', '.', '그', '##래', '##서', 'jason', '##에게', 'ᄉ', '##ᅡ', '##과', '##를', '했다']\n"
     ]
    }
   ],
   "source": [
    "sentence = '나는 책상 위에 사과를 먹었다. 알고 보니 그 사과는 Jason 것이었다. 그래서 Jason에게 사과를 했다'       \n",
    "print(tokenizer.tokenize(sentence))\n",
    "#multilingual model에는 한국어 데이터도 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d29fa",
   "metadata": {},
   "source": [
    "## Word-Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabd9cc",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "276f25bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 0, '책상': 1, '위에': 2, '사과를': 3, '먹었다': 4, '알고': 5, '보니': 6, '그': 7, '사과는': 8, 'Jason': 9, '것이었다': 10, '그래서': 11, 'Jason에게': 12, '했다': 13}\n"
     ]
    }
   ],
   "source": [
    "S1 = '나는 책상 위에 사과를 먹었다'        \n",
    "S2 = '알고 보니 그 사과는 Jason 것이었다'  \n",
    "S3 = '그래서 Jason에게 사과를 했다'       \n",
    "\n",
    "token2idx = {}\n",
    "index = 0\n",
    "\n",
    "for sentence in [S1, S2, S3]:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token2idx.get(token) == None:\n",
    "            token2idx[token] = index\n",
    "            index += 1\n",
    "\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e41104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t0\t나는\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t1\t책상\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t2\t위에\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t3\t사과를\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t4\t먹었다\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\t5\t알고\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\t6\t보니\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\t7\t그\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\t8\t사과는\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\t9\tJason\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\t10\t것이었다\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\t11\t그래서\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\t12\tJason에게\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\t13\t했다\n"
     ]
    }
   ],
   "source": [
    "#python list를 이용해 모든 token을 원-핫 인코딩으로 표현하는 방법\n",
    "V = len(token2idx) #14\n",
    "\n",
    "token2vec = [([0 if i != idx else 1 for i in range(V)], idx, token) for token, idx in token2idx.items() ]\n",
    "\n",
    "for x in token2vec:\n",
    "    print(\"\\t\".join([str(y) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "638738a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 책상 위에 사과를 먹었다 : \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "알고 보니 그 사과는 Jason 것이었다 : \n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "그래서 Jason에게 사과를 했다 : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#python numpy를 이용해 문장을 원-핫 인코딩으로 바꾸는 방법\n",
    "import numpy as np\n",
    "\n",
    "for sentence in [S1, S2, S3]:\n",
    "    onehot_s = []\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token2idx.get(token) != None:\n",
    "            vector = np.zeros((1,V))\n",
    "            vector[:,token2idx[token]] = 1\n",
    "            onehot_s.append(vector)\n",
    "        else:\n",
    "            print(\"UNK\")\n",
    "\n",
    "    print(f\"{sentence} : \")        \n",
    "    print(np.concatenate(onehot_s, axis = 0))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840fac0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice[py-3.7]",
   "language": "python",
   "name": "practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
